# 自我介绍

面试官您好，我是李睿杰，目前就读于北京邮电大学计算机学院，电子信息专业，现在是研二下学期。本科是北京邮电大学自动化学院，测控技术与仪器专业。

研究生阶段一直在导师的公司东信北邮参与项目，参与的项目主要是流媒体处理相关，主要项目是多人视频会议系统，还参与过一些视频和图像处理的项目，去年开始也接触过一些人脸关键点检测落地的项目。





# 视频会议系统介绍：

这个项目是从2020年开始的项目，用c++开发的多人视频会议系统，支持多人语音通话，多人摄像头画面共享以及屏幕共享。本项目是参考WebRTC架构以及根据标准RFC文档规范进行设计和开发的。整个会议系统由pc客户端，业务服务器，媒体转发服务器和媒体混流服务器构成。

目前系统支持两种会议模式，一种SFU模式，这种模式下媒体转发服务器收到客户端的媒体流后，直接转发给他对应的客户端。多路音视频的处理放在了客户端。MCU模式下媒体转发服务器将所有客户端的视频流发给混流服务器，混流服务器将混好的音频和视频流传回给媒体转发服务器，媒体转发服务器再转发给对应客户端。

目前这个系统能够支持我们实验室成员20人的日常会议。

我主要参与的是信令服务器和视频处理模块的开发。业务服务器主要处理所有用户的业务信息，协调转发和混流服务器。视频处理模块主要是利用ffmpeg实现视频的编解码，nalu的封装和解封装以及多路视频流的混合。也曾经参与过媒体转发服务器的开发，主要是接收客户端的音视频包，并根据媒体类型转发给媒体混流服务器。





## 业务服务器

客户端通过http请求登陆到业务服务器并获取到一个的token，并用这个token建立与业务服务器的webSocket连接，后续会议过程中的业务消息，比如加入会议，退出会议，管理信息都是通过webSocket连接进行通信。业务服务器与媒体转发和混流服务器之间采用http通信，如果由用户加入或者退出会议，或者开关媒体共享，业务服务器会将这些操作通知到媒体服务器。







## 视频处理模块

视频处理模块是从原来的客户端和混流服务器上将视频处理相关的工作提取出来，单独开发，最后作为一个模块集成到客户端和混流服务器。主要的功能是视频编解码，混合和缩放等





#### 亮点：

根据实际运用，定义了一系列api，可以实现客户端和媒体服务器再不用知道ffmpeg的情况下，完成视频处理。

在视频拼接时，对比ffmpeg拼接、opencv的拼接，以及直接操作像素点三种方式，最后采用直接拼接像素点，这种方式的效率更高。



#### 遇到的困难：

##### 内存泄漏

ffmpeg中存储视频画面帧的数据结构叫avframe，我们在编码或者处理前需要将采集到的原始数据的起始指针填充到avframe的指针数组data中再进行后续操作，处理完后需要将avframe的内存释放掉。之前在开发的过程中发现视频处理过程中总是存在内存泄漏，后来排查发现是因为释放avframe时，填充进去的视频数据不会同时被释放掉，需要手动释放掉这块数据

查看ffmpeg源码后发现是因为avframe在申请内存的时候分了两步，第一步只是分配一个结构体的空间，结构体里的数据部分是没有的。第二步填充视频数据时，需要用户填充或者调用另一个接口来填充默认值，如果填充默认数据，申请的另一块内存空间的起始地址存放到的avframe的buf部分，再把buf部分的指针存放到data。而释放的时候也是根据buf里的数组来释放的，如果是我们自己填充的数据，只会改变data数组，不会影响到buf数组，所以释放的时候因为buf数组没有值，就不会释放掉我们自己填充的data，导致内存泄漏

##### 解码后画面倾斜

显示画面的时候是将data中的数据按照每一帧的宽度来取，有些时候这样取出来的话会导致画面是斜着的，就是每一行像素会比上一行像素错开几个像素。

后来查看源码中的注释才知道是由于data数组中对内存做了优化，在每一行像素结尾可能会有个填充对齐，提高读取内存时的效率。所以data中存的数据的每行宽度和实际画面宽度可能不一致。而填充对齐后每一行数据的实际宽度会存在linesize的数据结构中，读取的时候按照这个值读取，并将填充的值去掉，就不会出现画面倾斜的情况了





## 媒体转发服务器

由业务服务器告知媒体转发服务器会议房间信息和会议中的用户信息，主要是通过rtp数据包中的ssrc字段区分客户端。由一个线程做接收和解析数据处理，将收到的队列放到相应的数据队列里，再由另一个线程从这个队列里取出数据发送给出去。



#### 亮点：

采用生产者消费者模型，但是为了保证实时性，生产者的线程一直需要向队列里存入数据，如果数据量超出了队列最大限度，就将队列前面的数据丢弃，保证音视频数据的实时性。

#### 困难：

在关闭房间时，偶尔出现程序挂掉。后来发现是由于房间管理的对象析构了，接收线程同时在往这个对象里填数据，后来加了一个互斥锁解决这个问题



# 3D头像原型系统

利用人脸关键点算法检测摄像头采集到的画面，将摄像头画面放入urho3d游戏引擎中的画布上，并将照相机的位置调整到刚好放下整个画布。在画布和照相机之间放上一个3D头像，让这个3d头像模型的位置和大小刚好能覆盖人脸的位置和大小。最后通过照相机显示3d空间的画面，完成在电脑摄像头画面基础上增加虚拟头像。



#### 亮点：

落地人脸检测算法，并封装，采用多线程分别进行摄像头画面采集，人脸检测和关键点识别。



#### 困难：

##### 降低时延，提高帧率

原先的两步采用单线程串行，比较耗时，后来改用每个线程内部保存一个变量，每次根据上一个线程的结果更新本线程的变量。

具体来说，共有摄像头采集，人脸检测和关键点识别三个线程，每个线程维护一个变量，采集线程按照25fps的帧率采集画面，每次采集到的画面都存放到一个变量中，每次采集都会覆盖掉上一次采集的结果。人脸检测每次从采集线程中取到当前时刻的画面数据，进行人脸检测，将人脸的区域信息放到检测线程中的一个变量中。关键点识别的将当前时刻摄像头采集的画面和当前时刻人脸区域信息来检测关键点

实验发现影响帧率和时延的主要原因是在人脸检测这一块，这块的帧率较低，改进后整个模块输出的帧率就取决于关键点检测的帧率，使整个模块效率提高，而且因为输出的时候用的是最近一次采集的画面，所以从采集到渲染的时延也有所降低。



# 彩铃质量检测系统

根据手机拨号时的视频彩铃录屏，与视频彩铃原视频内容进行对比，判断手机播放彩铃时是否出现卡顿，花屏等播放质量问题。

#### 亮点：

利用opencv对原视频和录屏视频画面序列进行对比，在满足一定条件时判断视频含有卡顿。

在检测是否有拨号盘遮挡，将录屏视频和原始视频逐像素相减，有遮挡的地方相减的值会比0大很多，由此判断是否存在遮挡